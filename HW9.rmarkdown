---
title: "HW9"
format: html
editor: visual
---

```{r}


library(tidyr)
library(dplyr)
library(readr)
library(lubridate)
library(psych)
library(tidymodels)
library(stats)
library(rsample)
library(yardstick)
library(tidyverse)
library(corrr)
library(parsnip)
library(tune)
library(glmnet)
library(baguette)
library(ranger)
```



## Reading in Data 


```{r}
data<-readr::read_csv("SeoulBikeData.csv",locale=locale(encoding="latin1"))
```



## Checking for Missing Values
### After checking to see if how many missing values we have in each columns we can see that there aren't any missing values. 

```{r}
sum_na<-function(column){
  sum(is.na(column))
}

na_counts<-data|>
  summarize(across(everything(),sum_na))
print(na_counts)

```

## Exploring Basic Summaries/Values
### One of the things that sticks out to me is that each variable has the same number of observations which confirms there aren't any missing values. We can also see which variables are numeric or not. Finally I can see that I'll need to rename the variables to make them more user friendly. 

```{r}
psych::describe(data)
```

## Here we are updating our data set.

```{r}
#Fixing the Date variable
data$Date<- dmy(data$Date)


#Here we are changing our categorical variables to be factors.
data<-data|>
  mutate(Seasons=as.factor(Seasons),
         Holiday=as.factor(Holiday),
         `Functioning Day`=as.factor(`Functioning Day`))

#Here we are updating the varaible names to make them more user friendly. 

data<-data|>
  rename(temp=`Temperature(°C)`,hr=Hour,humidity=`Humidity(%)`,rbc=`Rented Bike Count`,ws=`Wind speed (m/s)`,vis=`Visibility (10m)`,dpttemp=`Dew point temperature(°C)`,solrad=`Solar Radiation (MJ/m2)`,rain=`Rainfall(mm)`,snow=`Snowfall (cm)`,holiday=Holiday,season=Seasons,funday=`Functioning Day`)

```

## Creating Summaries Across Categorical variables
# I noticed that no bike rentals happened when Functional Day equals No, so I subset the data to only have Functional Day equals Yes. 


```{r}
data|>
  group_by(funday,holiday,season)|>
  filter(funday=="Yes")|>
  summarise(across(where(is.numeric),
                   list("sum"=sum,"mean"=mean,"median"=median,"sd"=sd,"min"=min,"max"=max),
                   .names="{.fn}_{.col}"))

```

## Manipulating the original data set that we will use for analysis.


```{r}
Model_data<-data|>
  group_by(Date,season,holiday)|>
   summarize(
     sum_rbc=sum(rbc,na.rm=TRUE),
     sum_rain=sum(rain,na.rm=TRUE),
     sum_snow=sum(snow,na.rm=TRUE),
     mean_temp=mean(temp,na.rm=TRUE),
     mean_hum=mean(humidity,na.rm=TRUE),
     mean_ws=mean(ws,na.rm=TRUE),
     mean_vis=mean(vis,na.rm=TRUE),
     mean_dpttemp=mean(dpttemp,na.rm=TRUE),
     mean_solrad=mean(solrad,na.rm=TRUE),
   )|>
  ungroup()
   

```


## Recreating the summaries from above with the new model data.


```{r}
Model_data|>
  group_by(holiday,season)|>
  summarise(across(where(is.numeric),
                   list("sum"=sum,"mean"=mean,"median"=median,"sd"=sd,"min"=min,"max"=max),
                   .names="{.fn}_{.col}"))

```


## Finding the correlation between all numeric variables grouped by holiday and season

```{r}
Correlation <- Model_data |>
   group_by(holiday,season)|>
  correlate()
```



## Here is a visual representation for the number of holidays broken down by season


```{r}
g <- ggplot(data = Model_data , aes(x = season, fill = holiday))
g + geom_bar()+
 labs(x = "Season")
```

## Here I thought it was interesting to see the relationship between the temperture, season, and bike rentals.

```{r}
g <- ggplot(Model_data,
 aes(x = mean_temp, y = sum_rbc, color = season))
g + geom_point()
```


## Splitting the data on a 75/25 split.


```{r}
data_split<-initial_split(Model_data,strata=season,prop=0.75)
data_training<-training(data_split)
data_test<-testing(data_split)
data_fold<-vfold_cv(data_training,10)

print(data_training)
print(data_test)
```


## Creating a 10 fold cv split on the training set


```{r}
get_cv_splits <- function(data, num_folds){
  #get fold size
  size_fold <- floor(nrow(data)/num_folds)
  #get random indices to subset the data with
  random_indices <- sample(1:nrow(data), size = nrow(data), replace = FALSE)
  #create a list to save our folds in
  folds <- list()
  #now cycle through our random indices vector and take the appropriate observations to each fold
  for(i in 1:num_folds){
    if (i < num_folds) {
      fold_index <- seq(from = (i-1)*size_fold +1, to = i*size_fold, by = 1)
      folds[[i]] <- data[random_indices[fold_index], ]
    } else {
      fold_index <- seq(from = (i-1)*size_fold +1, to = length(random_indices), by = 1)
      folds[[i]] <- data[random_indices[fold_index], ]
    }
  }
  return(folds)
}
folds <- get_cv_splits(data_training, 10)

```



## Creating a model for recipe 1

```{r}
rec1<-recipe(sum_rbc~.,data=data_training)|>
  step_date(Date,features="dow")|>
  step_mutate(DOW=factor(if_else(Date_dow %in% c("Sat","Sun"),"Weekend","Weekday")))|>
  step_rm(Date,Date_dow)|>
  step_dummy(season,holiday,DOW)|>
  step_normalize(all_numeric(),-all_outcomes())
 


```




## Creating a model for recipe 2


```{r}
rec2<-recipe(sum_rbc~.,data=data_training)|>
  update_role(Date,new_role = "ID")|>
  step_date(Date,features="dow")|>
 step_mutate(DOW=factor(if_else(Date_dow %in% c("Sat","Sun"),"Weekend","Weekday")))|>
  step_rm(Date,Date_dow)|>
  step_dummy(season,holiday,DOW)|>
  step_normalize(all_numeric(),-all_outcomes())|>
  step_interact(terms = ~starts_with("holiday")*starts_with("season")+mean_temp*sum_rain+mean_temp*starts_with("season"))

```



## Creating a model for recipe 3

```{r}
rec3<-recipe(sum_rbc~.,data=data_training)|>
  update_role(Date,new_role = "ID")|>
  step_date(Date,features="dow")|>
 step_mutate(DOW=factor(if_else(Date_dow %in% c("Sat","Sun"),"Weekend","Weekday")))|>
  step_rm(Date,Date_dow)|>
  step_dummy(season,holiday,DOW)|>
  step_normalize(all_numeric(),-all_outcomes())|>
  step_interact(terms = ~starts_with("holiday")*starts_with("season")+mean_temp*sum_rain+mean_temp*starts_with("season"))|>
  step_poly(mean_temp,mean_ws,mean_vis,mean_dpttemp,mean_solrad,sum_rain,sum_snow,degree=2)
  

```




## Model for Recipe 1

```{r}
model<-linear_reg() %>%
  set_engine("lm")


cvFit1<-workflow()|>
  add_recipe(rec1)|>
  add_model(model)|>
  fit_resamples(data_fold)

cvFit2<-workflow()|>
  add_recipe(rec2)|>
  add_model(model)|>
  fit_resamples(data_fold)

cvFit3<-workflow()|>
  add_recipe(rec3)|>
  add_model(model)|>
  fit_resamples(data_fold)

metrics<-rbind(cvFit1 |> collect_metrics(),
      cvFit2 |> collect_metrics(),
      cvFit3 |> collect_metrics()
  
)

ffit<-workflow()|>
  add_recipe(rec1)|>
  add_model(model)|>
  last_fit(data_split)

ffit|>
  collect_metrics()

ffit|>
  extract_fit_parsnip()|>
  tidy()

```



## LASSO Model


```{r}

library(shape)

LASSO<-linear_reg(penalty = tune(),mixture=1)|>
  set_engine("glmnet")

LASSO_wkf<- workflow()|>
  add_recipe(rec1)|>
  add_model(LASSO)
  LASSO_wkf
  
  
  LASSO_grid<-LASSO_wkf|>
    tune_grid(resamples = data_fold,
              grid=grid_regular(penalty(),levels=200))
  
  LASSO_grid
  
  #A warning will occur for one value of the tuning parameter, safe to ignore
LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = data_fold,
            grid = grid_regular(penalty(), levels = 200)) 

LASSO_grid

LASSO_grid[1, ".metrics"][[1]]

LASSO_grid |>
  collect_metrics() |>
  filter(.metric == "rmse")

LASSO_grid |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line()


lowest_rmse <- LASSO_grid |>
  select_best(metric = "rmse")
lowest_rmse


LASSO_wkf |>
  finalize_workflow(lowest_rmse)

#fit it to the entire training set to see the model fit
LASSO_final <- LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  fit(data_training)
tidy(LASSO_final)


LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  last_fit(data_split) |>
  collect_metrics()



LASSO_final |>
  predict(data_test) |>
  pull() |>
  rmse_vec(truth = data_test$sum_rbc)

```


## Regression Tree Model

```{r}
tree_mod <- decision_tree(tree_depth = tune(),
                          min_n = 20,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("regression")


tree_wkf <- workflow() |>
  add_recipe(rec1) |>
  add_model(tree_mod)

temp <- tree_wkf |> 
  tune_grid(resamples = data_fold)
temp |> 
  collect_metrics()

tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(10, 5))

tree_fits <- tree_wkf |> 
  tune_grid(resamples = data_fold,
            grid = tree_grid)
tree_fits

tree_fits |>
  collect_metrics()

tree_fits %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(linewidth = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)

tree_fits |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  arrange(mean)

tree_best_params <- select_best(tree_fits)
tree_best_params

tree_final_wkf <- tree_wkf |>
  finalize_workflow(tree_best_params)

tree_final_fit <- tree_final_wkf |>
  last_fit(data_split)
tree_final_fit

tree_final_fit |>
  collect_metrics()

tree_final_model <- extract_workflow(tree_final_fit) 
tree_final_model


```


## Bagged Tree Model

```{r}
bag_spec <- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |>
 set_engine("rpart") |>
 set_mode("regression")



bag_wkf <- workflow() |>
 add_recipe(rec1) |>
 add_model(bag_spec)

bag_fit <- bag_wkf |>
 tune_grid(resamples = data_fold,
 grid = grid_regular(cost_complexity(),
 levels = 15),
 metrics = metric_set(rmse))
bag_fit

bag_fit |>
 collect_metrics() |>
 filter(.metric == "rmse") |>
 arrange(mean)

bag_best_params <- select_best(bag_fit)
bag_best_params

bag_final_wkf <- bag_wkf |>
 finalize_workflow(bag_best_params)
bag_final_fit <- bag_final_wkf |>
 last_fit(data_split, metrics = metric_set(rmse))

bag_final_fit|>collect_metrics()

```


## Random Forest Model


```{r}


rf_spec <- rand_forest(mtry = tune()) |>
 set_engine("ranger") |>
 set_mode("regression")

rf_wkf <- workflow() |>
 add_recipe(rec1) |>
 add_model(rf_spec)

rf_fit <- rf_wkf |>
 tune_grid(resamples = data_fold,
 grid = 7,
 metrics = metric_set(rmse))

rf_fit |>
 collect_metrics() |>
 filter(.metric == "rmse") |>
 arrange(mean)

rf_best_params <- select_best(rf_fit)
rf_best_params

rf_final_wkf <- rf_wkf |>
 finalize_workflow(rf_best_params)
rf_final_fit <- rf_final_wkf |>
 last_fit(data_split, metrics = metric_set(rmse))

rf_final_fit |> collect_metrics()
```

